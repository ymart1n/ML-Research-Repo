```{r}
data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
set.seed(17)

## view the first few rows of the data
head(mydata)
summary(mydata)

## split training set and testing test
train_sub = sample(nrow(mydata),0.75*nrow(mydata))

train_data = mydata[train_sub,]
test_data = mydata[-train_sub,]
library(randomForest)

## calculate mean of model's error rate based on OOB data
err<-as.numeric()
for(i in 1:((length(names(train_data)))-1)){
  print(i)
  mtry_test <- randomForest(as.factor(train_data$admit)~.,data=train_data,mtry=i,ntree=1000)
  err<- append( err, mean( mtry_test$err.rate ) )
}
print(err)

##obtain number of variables for a decision tree that result in the minimum error rate
mtry <- which.min(err)
print(mtry)
ntree_fit <- randomForest(as.factor(train_data$admit)~., data=train_data, mtry=mtry, ntree=1000)
plot(ntree_fit)

## then we get that when number of trees=900, the error is relatively stable
```
```{r}
rf<-randomForest(as.factor(train_data$admit)~., data=train_data, mtry=mtry, ntree=900, importance=T )

## show brief info about the random forest
print(rf) 

## show accuracy and gini coefficient
importance(rf)
varImpPlot(rf)

## show number of nodes for each decision treee
hist(treesize(rf))
max(treesize(rf));min(treesize(rf))

pred1<-predict(rf,data=train_data)

Freq1<-table(pred1,train_data$admit)

## prediction accuracy
sum(diag(Freq1))/sum(Freq1)
```

